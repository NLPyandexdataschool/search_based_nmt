{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upper_folder(path):\n",
    "    return '/'.join(path.split('/')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dev_size = 0.002 # to have about 1k\n",
    "add_to_dev = 5000\n",
    "split_size = 0.5\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "p = !pwd\n",
    "p = get_upper_folder(p[0])\n",
    "\n",
    "data_dir = os.path.join(p, 'search_based_nmt', 'data', 'raw_data')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.dev.txt          en.train_ost.txt    he.train.txt        hewv.train.txt\r\n",
      "en.new_dev.txt      en.train_search.txt he.train_dev.txt    \u001b[34mtrain_half\u001b[m\u001b[m\r\n",
      "en.new_test.txt     he.dev.txt          he.train_ost.txt    \u001b[34mtrain_no_search\u001b[m\u001b[m\r\n",
      "en.test.txt         he.new_dev.txt      he.train_search.txt \u001b[34mtrain_octa\u001b[m\u001b[m\r\n",
      "en.train.txt        he.new_test.txt     hewv.dev.txt        \u001b[34mtrain_quarter\u001b[m\u001b[m\r\n",
      "en.train_dev.txt    he.test.txt         hewv.test.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_translations(part):\n",
    "    translations = defaultdict(list)\n",
    "\n",
    "    with open(os.path.join(data_dir, 'en.{}.txt'.format(part))) as f:\n",
    "        en = [l.strip() for l in f]\n",
    "\n",
    "    with open(os.path.join(data_dir, 'he.{}.txt'.format(part))) as f:\n",
    "        he = [l.strip() for l in f]\n",
    "\n",
    "    for x,y in zip(he, en):\n",
    "        translations[x].append(y)\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations_train = get_translations('train')\n",
    "translations_dev = get_translations('dev')\n",
    "translations_test = get_translations('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 296213\n",
      "test size 986\n",
      "dev size 3963\n"
     ]
    }
   ],
   "source": [
    "print ('train size', len(translations_train))\n",
    "print ('test size', len(translations_test))\n",
    "print ('dev size', len(translations_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intersection of train and dev: 2216\n",
      "intersection of train and test: 18\n"
     ]
    }
   ],
   "source": [
    "print ('intersection of train and dev:', len([k for k in translations_dev if k in translations_train]))\n",
    "print ('intersection of train and test:', len([k for k in translations_test if k in translations_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = list(translations_train.keys())\n",
    "dev_words = [k for k in translations_dev if k not in translations_train]\n",
    "test_words = [k for k in translations_test if k not in translations_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev words with no intersections 1747\n",
      "test words with no intersections 968\n"
     ]
    }
   ],
   "source": [
    "print ('dev words with no intersections', len(dev_words))\n",
    "print ('test words with no intersections', len(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([215745.,  43781.,  15404.,   7402.,   3983.,   2608.,   1776.,\n",
       "          1262.,    858.,    648.]),\n",
       " array([ 1. ,  1.9,  2.8,  3.7,  4.6,  5.5,  6.4,  7.3,  8.2,  9.1, 10. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113f0fcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(v) for k, v in translations_train.items()], range=(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = []\n",
    "for w in dev_words:\n",
    "    for translation in translations_dev[w]:\n",
    "        dev_data.append((w, translation))\n",
    "\n",
    "np.random.shuffle(dev_data)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.clear_dev.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in dev_data]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.clear_dev.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in dev_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for w in test_words:\n",
    "    for translation in translations_test[w]:\n",
    "        test_data.append((w, translation))\n",
    "\n",
    "np.random.shuffle(test_data)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.clear_test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in test_data]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.clear_test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in test_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train by keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = list(translations_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296213"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for train_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_dev = train_test_split(train_words, test_size=train_dev_size, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train_dev 975\n",
      "len of ost train 495025\n"
     ]
    }
   ],
   "source": [
    "print ('len of train_dev', sum([len(translations_train[k]) for k in train_dev]))\n",
    "print ('len of ost train', sum([len(translations_train[k]) for k in train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_data = []\n",
    "for w in train_dev:\n",
    "    for translation in translations_train[w]:\n",
    "        train_dev_data.append((w, translation))\n",
    "\n",
    "np.random.shuffle(train_dev_data)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.train_dev.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in train_dev_data]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.train_dev.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in train_dev_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_search = train_test_split(train, test_size=split_size, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train_search 246794\n",
      "len of ost train 248231\n"
     ]
    }
   ],
   "source": [
    "print ('len of train_search', sum([len(translations_train[k]) for k in train_search]))\n",
    "print ('len of ost train', sum([len(translations_train[k]) for k in train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make extra dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10222"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_search, extra_dev_words = train_test_split(train_search, test_size=add_to_dev, random_state=random_seed)\n",
    "\n",
    "extra_dev = []\n",
    "for w in extra_dev_words:\n",
    "    for translation in translations_train[w]:\n",
    "        extra_dev.append((w, translation))\n",
    "extra_dev += dev_data\n",
    "len(extra_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(extra_dev)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.mode_dev.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in extra_dev]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.more_dev.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in extra_dev]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make extra test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9372"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_search, extra_test_words = train_test_split(train_search, test_size=add_to_dev, random_state=random_seed)\n",
    "\n",
    "extra_test = []\n",
    "for w in extra_test_words:\n",
    "    for translation in translations_train[w]:\n",
    "        extra_test.append((w, translation))\n",
    "extra_test += test_data\n",
    "len(extra_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(extra_test)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.mode_test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in extra_test]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.more_test.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in extra_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save search data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_search_data = []\n",
    "for w in train_search:\n",
    "    for translation in translations_train[w]:\n",
    "        train_search_data.append((w, translation))\n",
    "\n",
    "np.random.shuffle(train_search_data)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.train_search.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in train_search_data]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.train_search.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in train_search_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save another part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_search_data = []\n",
    "for w in train:\n",
    "    for translation in translations_train[w]:\n",
    "        train_no_search_data.append((w, translation))\n",
    "\n",
    "np.random.shuffle(train_no_search_data)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.train_no_search.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in train_no_search_data]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.train_no_search.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in train_no_search_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_half = train_test_split(train, test_size=split_size, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train_half 124418\n",
      "len of ost train 123813\n"
     ]
    }
   ],
   "source": [
    "print ('len of train_half', sum([len(translations_train[k]) for k in train_half]))\n",
    "print ('len of ost train', sum([len(translations_train[k]) for k in train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_half_data = []\n",
    "for w in train_half:\n",
    "    for translation in translations_train[w]:\n",
    "        train_half_data.append((w, translation))\n",
    "\n",
    "np.random.shuffle(train_half_data)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.train_half.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in train_half_data]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.train_half.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in train_half_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_quarter = train_test_split(train, test_size=split_size, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train_quarter 62473\n",
      "len of ost train 61340\n"
     ]
    }
   ],
   "source": [
    "print ('len of train_quarter', sum([len(translations_train[k]) for k in train_quarter]))\n",
    "print ('len of ost train', sum([len(translations_train[k]) for k in train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_quarter_data = []\n",
    "for w in train_quarter:\n",
    "    for translation in translations_train[w]:\n",
    "        train_quarter_data.append((w, translation))\n",
    "\n",
    "np.random.shuffle(train_quarter_data)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.train_quarter.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in train_quarter_data]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.train_quarter.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in train_quarter_data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split octa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_octa = train_test_split(train, test_size=split_size, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train_octa 30664\n",
      "len of ost train 30676\n"
     ]
    }
   ],
   "source": [
    "print ('len of train_octa', sum([len(translations_train[k]) for k in train_octa]))\n",
    "print ('len of ost train', sum([len(translations_train[k]) for k in train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_octa_data = []\n",
    "for w in train_octa:\n",
    "    for translation in translations_train[w]:\n",
    "        train_octa_data.append((w, translation))\n",
    "\n",
    "np.random.shuffle(train_octa_data)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.train_octa.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in train_octa_data]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.train_octa.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in train_octa_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ost_data = []\n",
    "for w in train:\n",
    "    for translation in translations_train[w]:\n",
    "        train_ost_data.append((w, translation))\n",
    "\n",
    "np.random.shuffle(train_ost_data)\n",
    "\n",
    "with open(os.path.join(data_dir, 'he.train_ost.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[0] for x in train_ost_data]))\n",
    "    \n",
    "with open(os.path.join(data_dir, 'en.train_ost.txt'), 'w') as f:\n",
    "    f.write('\\n'.join([x[1] for x in train_ost_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.dev.txt                             he.test.txt\r\n",
      "en.new_dev.txt                         he.train.txt\r\n",
      "en.new_test.txt                        he.train_dev.txt\r\n",
      "en.test.txt                            he.train_half.txt\r\n",
      "en.train.txt                           he.train_no_search.txt\r\n",
      "en.train_dev.txt                       he.train_octa.txt\r\n",
      "en.train_half.txt                      he.train_ost.txt\r\n",
      "en.train_no_search.txt                 he.train_quarter.txt\r\n",
      "en.train_octa.txt                      he.train_search.txt\r\n",
      "en.train_ost.txt                       he2en_ws-dev-00000-of-00001\r\n",
      "en.train_quarter.txt                   he2en_ws-train-00000-of-00001\r\n",
      "en.train_search.txt                    hewv.dev.txt\r\n",
      "he-to-en.translit.results.txt          hewv.test.txt\r\n",
      "he.dev.txt                             hewv.train.txt\r\n",
      "he.new_dev.txt                         translit_he_to_en-dev-00000-of-00001\r\n",
      "he.new_test.txt                        translit_he_to_en-train-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "!ls $data_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boyalex/shad/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensor2tensor.data_generators import generator_utils\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import translate\n",
    "from tensor2tensor.utils import registry\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from tensor2tensor import models\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.layers import common_layers\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.utils import t2t_model\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import metrics\n",
    "\n",
    "# Enable TF Eager execution\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "# Other setup\n",
    "Modes = tf.estimator.ModeKeys\n",
    "\n",
    "# Setup some directories\n",
    "data_dir = os.path.expanduser(\"./data\")\n",
    "tmp_dir = os.path.expanduser(\"./tmp\")\n",
    "train_dir = os.path.expanduser(\"./train\")\n",
    "checkpoint_dir = os.path.expanduser(\"./checkpoints\")\n",
    "tf.gfile.MakeDirs(data_dir)\n",
    "tf.gfile.MakeDirs(tmp_dir)\n",
    "tf.gfile.MakeDirs(train_dir)\n",
    "tf.gfile.MakeDirs(checkpoint_dir)\n",
    "gs_data_dir = \"gs://tensor2tensor-data\"\n",
    "gs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('he-en/en.train.txt') as f, open('he-en/he.train.txt') as g, open('en_he.train.txt', 'w') as out:\n",
    "    for en, he in zip(f, g):\n",
    "        out.write(en.strip() + '\\t' + he.strip() + '\\n')\n",
    "        \n",
    "with open('he-en/en.dev.txt') as f, open('he-en/he.dev.txt') as g, open('en_he.dev.txt', 'w') as out:\n",
    "    for en, he in zip(f, g):\n",
    "        out.write(en.strip() + '\\t' + he.strip() + '\\n')\n",
    "        \n",
    "with open('he-en/en.test.txt') as f, open('he-en/he.test.txt') as g, open('en_he.test.txt', 'w') as out:\n",
    "    for en, he in zip(f, g):\n",
    "        out.write(en.strip() + '\\t' + he.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "\n",
    "with open('en_he.train.txt', 'r') as f:\n",
    "    all_data += [l.strip() for l in f]\n",
    "with open('en_he.dev.txt', 'r') as f:\n",
    "    all_data += [l.strip() for l in f]\n",
    "with open('en_he.test.txt', 'r') as f:\n",
    "    all_data += [l.strip() for l in f]\n",
    "\n",
    "with open('main_dataset.txt', 'w') as f:\n",
    "    f.write('\\n'.join(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bimdurot\tבמדורות\r\n",
      "sheyikrom\tשיקרום\r\n",
      "iter\tאיטר\r\n",
      "toses\tתוסס\r\n",
      "kvoe\tקב\r\n",
      "bshilya\tבשליא\r\n",
      "liftokh\tלפתוח\r\n",
      "shvutchem\tשבותכם\r\n",
      "mtse\tמצה\r\n",
      "mtserin\tמצרין\r\n"
     ]
    }
   ],
   "source": [
    "!head main_dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@registry.register_problem\n",
    "class HEENTranslitProblem(text_problems.Text2TextProblem):\n",
    "    filename = './main_dataset.txt'\n",
    "    name = 'heen_translit'\n",
    "    \n",
    "    @property\n",
    "    def is_generate_per_split(self):\n",
    "        return True\n",
    "    \n",
    "    def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "        \"\"\"Generate samples of input text and target text pairs.\n",
    "        Each yielded dict will be made into a single example. The values should be\n",
    "        raw text. The Problem will generate a vocabulary and encode the raw text as\n",
    "        integers as part of the data generation process.\n",
    "        This method is typically called once per split in `self.dataset_splits`\n",
    "        unless `self.is_generate_per_split=False`.\n",
    "        Args:\n",
    "          data_dir: final data directory. Typically only used in this method to copy\n",
    "            over user-supplied vocab files (for example, if vocab_type ==\n",
    "            VocabType.TOKEN).\n",
    "          tmp_dir: temporary directory that you can use for downloading and scratch.\n",
    "          dataset_split: problem.DatasetSplit, which data split to generate samples\n",
    "            for (for example, training and evaluation).\n",
    "        Yields:\n",
    "          {\"inputs\": text, \"targets\": text}\n",
    "        \"\"\"\n",
    "        with open(self.filename) as f:\n",
    "            for l in f:\n",
    "                en, he = l.strip().split('\\t')\n",
    "                yield {\n",
    "                    \"inputs\": he,\n",
    "                    \"targets\": en\n",
    "                }\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def vocab_type(self):\n",
    "        \"\"\"What kind of vocabulary to use.\n",
    "        `VocabType`s:\n",
    "          * `SUBWORD`: `SubwordTextEncoder`, an invertible wordpiece vocabulary.\n",
    "            Must provide `self.approx_vocab_size`. Generates the vocabulary based on\n",
    "            the training data. To limit the number of samples the vocab generation\n",
    "            looks at, override `self.max_samples_for_vocab`. Recommended and\n",
    "            default.\n",
    "          * `CHARACTER`: `ByteTextEncoder`, encode raw bytes.\n",
    "          * `TOKEN`: `TokenTextEncoder`, vocabulary based on a file. Must provide a\n",
    "            vocabulary file yourself (`TokenTextEncoder.store_to_file`) because one\n",
    "            will not be generated for you. The vocab file should be stored in\n",
    "            `data_dir/` with the name specified by `self.vocab_filename`.\n",
    "        Returns:\n",
    "          VocabType constant\n",
    "        \"\"\"\n",
    "        return text_problems.VocabType.CHARACTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "12\n",
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:49:47,479] Setting T2TModel mode to 'eval'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:49:47,485] Setting hparams.dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:49:47,490] Setting hparams.layer_prepostprocess_dropout to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:49:47,495] Setting hparams.symbol_dropout to 0.0\n"
     ]
    }
   ],
   "source": [
    "from tensor2tensor.utils import trainer_lib\n",
    "# data_dir = '.'\n",
    "\n",
    "\n",
    "model_name = \"lstm_seq2seq_attention\"\n",
    "hparams_set = \"lstm_attention\"\n",
    "\n",
    "hparams = trainer_lib.create_hparams(hparams_set, data_dir=data_dir, problem_name=\"heen_translit_problem\")\n",
    "print (hparams.max_length)\n",
    "hparams.set_hparam('max_length', 12)\n",
    "print (hparams.max_length)\n",
    "\n",
    "# NOTE: Only create the model once when restoring from a checkpoint; it's a\n",
    "# Layer and so subsequent instantiations will have different variable scopes\n",
    "# that will not match the checkpoint.\n",
    "translate_model = registry.model(model_name)(hparams, Modes.EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = HEENTranslitProblem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping generator because outputs files exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:49:47,621] Skipping generator because outputs files exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping generator because outputs files exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:49:47,629] Skipping generator because outputs files exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Skipping shuffle because output files exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:49:47,649] Skipping shuffle because output files exist\n"
     ]
    }
   ],
   "source": [
    "problem.generate_data(data_dir, tmp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from ./data/heen_translit_problem-train*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:49:47,679] Reading data files from ./data/heen_translit_problem-train*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:49:47,687] partition: 0 num_data_files: 100\n"
     ]
    }
   ],
   "source": [
    "@tfe.implicit_value_and_gradients\n",
    "def loss_fn(features):\n",
    "  _, losses = translate_model(features)\n",
    "  return losses[\"training\"]\n",
    "\n",
    "# Setup the training data\n",
    "BATCH_SIZE = 128\n",
    "train_dataset = problem.dataset(Modes.TRAIN, data_dir)\n",
    "train_dataset = train_dataset.repeat(None).batch(BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot batch tensors with different shapes in component 0. First element had shape [11] and element 1 had shape [17]. [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8a13101b1e8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNUM_STEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Make it 4D.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shad/lib/python3.6/site-packages/tensorflow/contrib/eager/python/datasets.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shad/lib/python3.6/site-packages/tensorflow/contrib/eager/python/datasets.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \"\"\"\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shad/lib/python3.6/site-packages/tensorflow/contrib/eager/python/datasets.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     return sparse.deserialize_sparse_tensors(\n",
      "\u001b[0;32m~/shad/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m    928\u001b[0m     _result = _execute.execute(b\"IteratorGetNextSync\", len(output_types),\n\u001b[1;32m    929\u001b[0m                                \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_inputs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_attrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                                name=name)\n\u001b[0m\u001b[1;32m    931\u001b[0m   _execute.record_gradient(\n\u001b[1;32m    932\u001b[0m       \"IteratorGetNextSync\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/shad/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shad/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot batch tensors with different shapes in component 0. First element had shape [11] and element 1 had shape [17]. [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 500\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(train_dataset)):\n",
    "  example[\"targets\"] = tf.reshape(example[\"targets\"], [BATCH_SIZE, 1, 1, 1])  # Make it 4D.\n",
    "  loss, gv = loss_fn(example)\n",
    "  optimizer.apply_gradients(gv)\n",
    "\n",
    "  if count % 50 == 0:\n",
    "    print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
    "  if count >= NUM_STEPS:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the problem\n",
    "# ende_problem = problems.problem(\"translate_ende_wmt32k\")\n",
    "# ende_problem.generate_data(data_dir, tmp_dir)\n",
    "\n",
    "# Copy the vocab file locally so we can encode inputs and decode model outputs\n",
    "# All vocabs are stored on GCS\n",
    "# vocab_file = os.path.join(gs_data_dir, \"vocab.ende.32768\")\n",
    "# !gsutil cp {vocab_file} {data_dir}\n",
    "\n",
    "# Get the encoders from the problem\n",
    "encoders = problem.feature_encoders(data_dir)\n",
    "\n",
    "# Setup helper functions for encoding and decoding\n",
    "def encode(input_str, output_str=None):\n",
    "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
    "  batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
    "  return {\"inputs\": batch_inputs}\n",
    "\n",
    "def decode(integers):\n",
    "  \"\"\"List of ints to str\"\"\"\n",
    "  integers = list(np.squeeze(integers))\n",
    "  if 1 in integers:\n",
    "    integers = integers[:integers.index(1)]\n",
    "  return encoders[\"inputs\"].decode(np.squeeze(integers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading data files from ./data/heen_translit_problem-train*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:56:31,674] Reading data files from ./data/heen_translit_problem-train*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-13 22:56:31,698] partition: 0 num_data_files: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs, encoded:\n",
      "[217, 151, 217, 150, 217, 149, 217, 166, 217, 150, 1]\n",
      "Inputs, decoded:\n",
      "והדפה\n",
      "Targets, encoded:\n",
      "[120, 106, 99, 102, 99, 104, 99, 106, 1]\n",
      "Targets, decoded:\n",
      "vhadafah\n"
     ]
    }
   ],
   "source": [
    "# Generate and view the data\n",
    "# This cell is commented out because WMT data generation can take hours\n",
    "\n",
    "# ende_problem.generate_data(data_dir, tmp_dir)\n",
    "example = tfe.Iterator(problem.dataset(Modes.TRAIN, data_dir)).next()\n",
    "inputs = [int(x) for x in example[\"inputs\"].numpy()] # Cast to ints.\n",
    "targets = [int(x) for x in example[\"targets\"].numpy()] # Cast to ints.\n",
    "\n",
    "\n",
    "\n",
    "# Example inputs as int-tensor.\n",
    "print(\"Inputs, encoded:\")\n",
    "print(inputs)\n",
    "print(\"Inputs, decoded:\")\n",
    "# Example inputs as a sentence.\n",
    "print(decode(inputs))\n",
    "# Example targets as int-tensor.\n",
    "print(\"Targets, encoded:\")\n",
    "print(targets)\n",
    "# Example targets as a sentence.\n",
    "print(\"Targets, decoded:\")\n",
    "print(decode(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dataset() missing 1 required positional argument: 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-52694481f630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: dataset() missing 1 required positional argument: 'mode'"
     ]
    }
   ],
   "source": [
    "problem.dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_mode(Modes.EVAL)\n",
    "mnist_eval_dataset = mnist_problem.dataset(Modes.EVAL, data_dir)\n",
    "\n",
    "# Create eval metric accumulators for accuracy (ACC) and accuracy in\n",
    "# top 5 (ACC_TOP5)\n",
    "metrics_accum, metrics_result = metrics.create_eager_metrics(\n",
    "    [metrics.Metrics.ACC, metrics.Metrics.ACC_TOP5])\n",
    "\n",
    "for count, example in enumerate(tfe.Iterator(mnist_eval_dataset)):\n",
    "  if count >= 200:\n",
    "    break\n",
    "\n",
    "  # Make the inputs and targets 4D\n",
    "  example[\"inputs\"] = tf.reshape(example[\"inputs\"], [1, 28, 28, 1])\n",
    "  example[\"targets\"] = tf.reshape(example[\"targets\"], [1, 1, 1, 1])\n",
    "\n",
    "  # Call the model\n",
    "  predictions, _ = model(example)\n",
    "\n",
    "  # Compute and accumulate metrics\n",
    "  metrics_accum(predictions, example[\"targets\"])\n",
    "\n",
    "# Print out the averaged metric values on the eval data\n",
    "for name, val in metrics_result().items():\n",
    "  print(\"%s: %.2f\" % (name, val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
